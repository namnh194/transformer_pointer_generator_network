{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8207083d-bb83-4615-bdf4-0fddc08cd9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refer: github.com/pbcquoc\n",
    "import torch, os, math, copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1313ca74-59a2-4b73-ae5a-42eb39121738",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Positional Encoder\n",
    "(this trick makes transformer and variants awesome but how ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fad86056-6cb8-45ec-babd-3b641de32c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 30, 512])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        return embed\n",
    "\n",
    "class PosisionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model=768, max_seq_len=256, dropout=0.1):\n",
    "        super(PosisionalEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0,d_model,2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** (2*i/d_model)))\n",
    "                pe[pos, i+1] = math.cos(pos / (10000 ** (2*i/d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # this makes pe is not trained/updated by optimizer\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        seq_len = x.size(1)\n",
    "        pe = Variable(self.pe[:, :seq_len], requires_grad=False)\n",
    "        if x.is_cuda:\n",
    "            pe.cuda()\n",
    "        x = self.dropout(x + pe)\n",
    "        return x\n",
    "PosisionalEncoder(512)(torch.rand(5, 30, 512)).shape    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a24b3-686c-45bd-b0c1-aaae84cbb9f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Multihead Attention operator\n",
    "(awesome feature extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a38eb69-a8ff-43c1-93e3-c58e8c55b3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 30, 512])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, dropout=None):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.dropout = nn.Dropout(dropout) if dropout else None\n",
    "        \n",
    "        # init mattrix weights for key, query and value\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        q: tensor shape `(batch_size, seq_len, d_model)`\n",
    "        k: tensor shape `(batch_size, seq_len, d_model)`\n",
    "        v: tensor shape `(batch_size, seq_len, d_model)`\n",
    "        mask: tensor shape `(batch_size, 1, seq_len)`, the mask of self-attn layer at Decoder\n",
    "        Return:\n",
    "        -------\n",
    "        output: tensor shape `(batch_size, seq_len, d_model)`\n",
    "        \"\"\"\n",
    "        # calculate query, key, value vector from weight mattrix\n",
    "        batch_size = q.size(0)\n",
    "        q = self.q_linear(q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.k_linear(k).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.v_linear(v).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # perfrom scale-dot attention op\n",
    "        score = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None: \n",
    "            mask = mask.unsqueeze(1)\n",
    "            score = score.masked_fill(mask==0, -1e9)\n",
    "        score = F.softmax(score, -1)\n",
    "        if self.dropout: \n",
    "            output = self.dropout(score)\n",
    "        output = torch.matmul(score, v)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.out(output)\n",
    "        return output\n",
    "\n",
    "MultiheadAttention(8, 512, 0.1)(torch.rand(8, 30, 512), torch.rand(8, 30, 512), torch.rand(8, 30, 512)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01c8e18-d1a1-47bf-a449-5a8d3ed4cce0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Residual connection and Layer normalization\n",
    "(faster converge and avoid losing information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd988189-8a46-4936-91c4-6cabd715dd24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 128, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(d_model))\n",
    "        self.bias = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm\n",
    "\n",
    "Norm(512)(torch.randn(8, 128, 512)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b937aaa2-4f13-4bf1-8f03-915c824bf435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 128, 512])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model=512, d_ff=2048, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model))\n",
    "    def forward(self, x):\n",
    "        out = self.ff(x)\n",
    "        return out\n",
    "FeedForward()(torch.randn(8,128,512)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe10e05-0d5b-4691-9e27-569f669d34e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Encoder, Decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5500d808-c2bd-4b9b-9a45-f146a3570fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 30, 512])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.attn = MultiheadAttention(n_heads, d_model, dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x: tensor shape `(batch_size, seq_len, model_dim)`\n",
    "        mask: tensor shape `(batch_size, 1, model_dim)` for mask self-attention\n",
    "        Return:\n",
    "        -------\n",
    "        out: tensor shape `(batch_size, seq_len, model_dim)`\n",
    "        \"\"\"\n",
    "        x_norm = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x_norm, x_norm, x_norm, mask))\n",
    "        x_norm = self.norm_2(x)\n",
    "        x = x = self.dropout_2(self.ff(x_norm))\n",
    "        return x\n",
    "net = EncoderBLock(512, 8, 2048)\n",
    "net(torch.randn(8, 30, 512), torch.randn(8, 1, 30)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7cd6d14d-ebe2-4bec-89ab-2045d67877ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 30, 512])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.norm_3 = Norm(d_model)\n",
    "        \n",
    "        self.attn_1 = MultiheadAttention(n_heads, d_model, dropout)\n",
    "        self.attn_2 = MultiheadAttention(n_heads, d_model, dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x: tensor input of target batch sentences\n",
    "            shape `(batch_size, seq_len, d_model)`\n",
    "        encoder_output: tensor output (contextual embedding) of encoder block\n",
    "            shape `(batch_size, seq_len, d_model)`\n",
    "        src_mask: tensor mask for encoder output\n",
    "            shape `(batch_size, 1, seq_len)`\n",
    "        tgt_mask: tensor for hide the future represented of predicted token from current step\n",
    "            shape `(batch_size, 1, seq_len)`\n",
    "        Return:\n",
    "        -------\n",
    "        out: tensor, contextual embedding of sentence\n",
    "            shape `(batch_size, seq_len, d_model)`\n",
    "        \"\"\"\n",
    "        x_norm = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn_1(x_norm, x_norm, x_norm, tgt_mask))\n",
    "        \n",
    "        # get corr between current token embedding of decoder with all token embedding from encoder\n",
    "        x_norm = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.attn_2(x_norm, encoder_output, encoder_output, src_mask))\n",
    "        \n",
    "        x_norm = self.norm_3(x)\n",
    "        x = x + self.dropout_3(self.ff(x_norm))\n",
    "        return x\n",
    "net = DecoderBlock(512, 8, 2048)\n",
    "net(torch.randn(8, 30, 512), torch.randn(8, 30, 512), torch.randn(8, 1, 30), torch.randn(8, 1, 30)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425985a5-3f34-4a18-8d34-4f43f3833852",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4831a114-849b-4e8a-a8b0-7a11862b5c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "271d551b-26cc-48b4-8164-63904735132e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 30, 512])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, n_heads, d_ff, num_layer, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.N = num_layer\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PosisionalEncoder(d_model, max_seq_len, dropout)\n",
    "        self.layers = get_clones(EncoderBlock(d_model, n_heads, d_ff, dropout), num_layer)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x: tensor, token idx of input sents\n",
    "            shape `(batch_size, seq_len)`\n",
    "        mask: tensor, shape `(batch_size, 1, seq_len)`\n",
    "        Return:\n",
    "        -------\n",
    "        out: tensor, shape `(batch_size, seq_len, d_model)`\n",
    "        \"\"\"\n",
    "        out = self.embed(x)\n",
    "        out = self.pe(out)\n",
    "        for i in range(self.N):\n",
    "            out = self.layers[i](out, mask)\n",
    "        out = self.norm(out)\n",
    "        return out\n",
    "en_vocab_size, max_seq_len, d_model, n_heads, d_ff, num_layer = 256, 30, 512, 8, 2048, 6\n",
    "net = Encoder(en_vocab_size, max_seq_len, d_model, n_heads, d_ff, num_layer)\n",
    "net(torch.LongTensor(8, max_seq_len).random_(0, en_vocab_size), torch.rand(8, 1, max_seq_len)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4bf6d4a7-169c-4d84-a6b6-7cd9d70b4d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 30, 512])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, n_heads, d_ff, num_layer, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.N = num_layer\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PosisionalEncoder(d_model, max_seq_len, dropout)\n",
    "        self.layers = get_clones(DecoderBlock(d_model, n_heads, d_ff, dropout), num_layer)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x: tensor, token idx of input sents\n",
    "            shape `(batch_size, seq_len)`\n",
    "        encoder_output: tensor, contextual embedding for input sents\n",
    "            shape `(batch_size, seq_len, d_model)`\n",
    "        src_mask, tgt_mask: tensor, shape `(batch_size, 1, seq_len)`\n",
    "            2 mask for sentence decoder and encoder respectively\n",
    "        Return:\n",
    "        -------\n",
    "        out: contextual embedding of whole predicted sents\n",
    "        \"\"\"\n",
    "        out = self.embed(x)\n",
    "        out = self.pe(out)\n",
    "        for i in range(self.N):\n",
    "            out = self.layers[i](out, encoder_output, src_mask, tgt_mask)\n",
    "        out = self.norm(out)\n",
    "        return out\n",
    "de_vocab_size, max_seq_len, d_model, n_heads, d_ff, num_layer = 256, 30, 512, 8, 2048, 6\n",
    "net = Decoder(de_vocab_size, max_seq_len, d_model, n_heads, d_ff, num_layer)\n",
    "net(torch.LongTensor(8, max_seq_len).random_(0, de_vocab_size), torch.rand(8, max_seq_len, d_model), torch.randn(8, 1, max_seq_len), torch.randn(8, 1, max_seq_len)).shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "865a00fa-0449-4cb9-8659-6a3e3b27fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, en_config, de_config):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(**en_config)\n",
    "        self.decoder = Decoder(**de_config)\n",
    "        self.fc = nn.Linear(de_config[\"d_model\"], de_config[\"vocab_size\"])\n",
    "    def forward(self, src_sent, tgt_sent, src_mask, tgt_mask):\n",
    "        encoder_output = self.encoder(src_sent, src_mask)\n",
    "        decoder_output = self.decoder(tgt_sent, encoder_output, src_mask, tgt_mask)\n",
    "        out = self.fc(decoder_output)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ae05c24d-3f2c-47e7-80b6-f8eec6415f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_config = {\n",
    "    \"vocab_size\": 256,\n",
    "    \"max_seq_len\": 30,\n",
    "    \"d_model\": 512,\n",
    "    \"n_heads\": 8,\n",
    "    \"d_ff\": 2048 ,\n",
    "    \"num_layer\": 6}\n",
    "de_config = {\n",
    "    \"vocab_size\": 128,\n",
    "    \"max_seq_len\": 18,\n",
    "    \"d_model\": 512,\n",
    "    \"n_heads\": 8,\n",
    "    \"d_ff\": 2048 ,\n",
    "    \"num_layer\": 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "fdb47aab-0db8-43d3-9aa4-8bd9c6c08140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44402816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 18, 128])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "en_seq_len = en_config[\"max_seq_len\"]\n",
    "de_seq_len = de_config[\"max_seq_len\"]\n",
    "en_vocab_size = en_config[\"vocab_size\"]\n",
    "de_vocab_size = de_config[\"vocab_size\"]\n",
    "\n",
    "net = Transformer(en_config, de_config)\n",
    "print(sum(p.numel() for p in net.parameters() if p.requires_grad))\n",
    "\n",
    "prob_map = net(src_sent=torch.LongTensor(batch_size, en_seq_len).random_(0, en_vocab_size),\\\n",
    "                tgt_sent=torch.LongTensor(batch_size, de_seq_len).random_(0, de_vocab_size),\\\n",
    "                src_mask=torch.randn(batch_size, 1, en_seq_len),\\\n",
    "                tgt_mask=torch.randn(batch_size, 1, de_seq_len))\n",
    "prob_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1496de33-2c84-44a4-a666-79201de77f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3601,  0.4778, -0.8045, -0.0144, -0.0967, -0.4138,  0.8579, -0.4447,\n",
       "         0.2281, -0.0091,  0.3790,  0.0720,  0.0519,  0.3575,  0.4493, -0.5355,\n",
       "        -0.6952,  0.1924,  0.0304,  0.0606, -0.2941, -0.3592, -1.0357,  0.4034,\n",
       "         0.2400,  0.2867, -1.2320,  0.2998,  0.2474,  0.1706, -0.6095,  0.1451,\n",
       "         0.5043, -0.0268, -0.0201, -0.3259, -0.4269, -0.0205,  0.2256,  0.0126,\n",
       "         0.3072, -0.7930, -0.7144,  0.4032,  0.1989,  0.8907, -0.8826, -0.3180,\n",
       "        -0.7040, -0.0209,  0.2044,  0.4262, -1.5077, -0.2173, -0.7608, -0.6945,\n",
       "         0.4776, -0.0646,  0.7162,  0.4895, -0.3851, -1.2172,  0.6286, -0.4064,\n",
       "        -0.2850, -0.3122,  0.7426,  0.7348,  0.5608,  0.2874, -0.4218,  0.2649,\n",
       "         0.0022,  1.0576,  0.6075,  0.7453,  0.4029,  0.8586, -0.2010,  0.4629,\n",
       "        -0.1588,  0.8472,  0.2970,  0.0262,  0.2182,  0.3083,  0.3395,  0.1802,\n",
       "         0.3352,  0.4177,  0.4831, -0.4374,  0.4496,  0.0138,  0.2590, -0.9276,\n",
       "         0.4266, -0.8178,  0.3704,  0.2342, -0.1715, -0.4856, -0.0321,  0.8518,\n",
       "         0.1903, -0.3803,  0.6393, -0.1543, -0.8921, -0.6320,  0.3057, -0.3967,\n",
       "         0.7446, -0.3819,  0.5180, -0.1374,  0.0588, -0.5606,  0.2305,  0.2372,\n",
       "        -0.7115,  0.7362,  0.0670,  0.6574,  0.9815,  0.4222, -0.3433, -0.6892],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_map[0,0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2cc2d9-b1ea-44cb-887a-624803855d99",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca3ce18-bd51-4d3b-9389-6a961b43721a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
