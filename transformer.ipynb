{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8207083d-bb83-4615-bdf4-0fddc08cd9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refer: github.com/pbcquoc\n",
    "import numpy as np\n",
    "import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch, os, math, copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1313ca74-59a2-4b73-ae5a-42eb39121738",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Positional Encoder\n",
    "(this trick makes transformer and variants awesome but how ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fad86056-6cb8-45ec-babd-3b641de32c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 30, 512])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        return embed\n",
    "\n",
    "class PosisionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model=768, max_seq_len=256, dropout=0.1):\n",
    "        super(PosisionalEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0,d_model,2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** (2*i/d_model)))\n",
    "                pe[pos, i+1] = math.cos(pos / (10000 ** (2*i/d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # this makes pe is not trained/updated by optimizer\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        seq_len = x.size(1)\n",
    "        pe = Variable(self.pe[:, :seq_len], requires_grad=False)\n",
    "        if x.is_cuda:\n",
    "            pe.cuda()\n",
    "        x = self.dropout(x + pe)\n",
    "        return x\n",
    "PosisionalEncoder(512)(torch.rand(5, 30, 512)).shape    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a24b3-686c-45bd-b0c1-aaae84cbb9f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Multihead Attention operator\n",
    "(awesome feature extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a38eb69-a8ff-43c1-93e3-c58e8c55b3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 30, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, dropout=None):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.dropout = nn.Dropout(dropout) if dropout else None\n",
    "        \n",
    "        # init mattrix weights for key, query and value\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        q: tensor shape `(batch_size, seq_len, d_model)`\n",
    "        k: tensor shape `(batch_size, seq_len, d_model)`\n",
    "        v: tensor shape `(batch_size, seq_len, d_model)`\n",
    "        mask: tensor shape `(batch_size, 1, seq_len)`, the mask of self-attn layer at Decoder\n",
    "        Return:\n",
    "        -------\n",
    "        output: tensor shape `(batch_size, seq_len, d_model)`\n",
    "        \"\"\"\n",
    "        # calculate query, key, value vector from weight mattrix\n",
    "        batch_size = q.size(0)\n",
    "        q = self.q_linear(q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.k_linear(k).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.v_linear(v).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # perfrom scale-dot attention op\n",
    "        score = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None: \n",
    "            mask = mask.unsqueeze(1)\n",
    "            score = score.masked_fill(mask==0, -1e9)\n",
    "        score = F.softmax(score, -1)\n",
    "        if self.dropout: \n",
    "            output = self.dropout(score)\n",
    "        output = torch.matmul(score, v)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.out(output)\n",
    "        return output\n",
    "\n",
    "MultiheadAttention(8, 512, 0.1)(torch.rand(8, 30, 512), torch.rand(8, 30, 512), torch.rand(8, 30, 512)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01c8e18-d1a1-47bf-a449-5a8d3ed4cce0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Residual connection and Layer normalization\n",
    "(faster converge and avoid losing information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd988189-8a46-4936-91c4-6cabd715dd24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 128, 512])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(d_model))\n",
    "        self.bias = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm\n",
    "\n",
    "Norm(512)(torch.randn(8, 128, 512)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b937aaa2-4f13-4bf1-8f03-915c824bf435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 128, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model=512, d_ff=2048, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model))\n",
    "    def forward(self, x):\n",
    "        out = self.ff(x)\n",
    "        return out\n",
    "FeedForward()(torch.randn(8,128,512)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe10e05-0d5b-4691-9e27-569f669d34e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Encoder, Decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5500d808-c2bd-4b9b-9a45-f146a3570fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 30, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.attn = MultiheadAttention(n_heads, d_model, dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x: tensor shape `(batch_size, seq_len, model_dim)`\n",
    "        mask: tensor shape `(batch_size, 1, model_dim)` for mask self-attention\n",
    "        Return:\n",
    "        -------\n",
    "        out: tensor shape `(batch_size, seq_len, model_dim)`\n",
    "        \"\"\"\n",
    "        x_norm = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x_norm, x_norm, x_norm, mask))\n",
    "        x_norm = self.norm_2(x)\n",
    "        x = x = self.dropout_2(self.ff(x_norm))\n",
    "        return x\n",
    "net = EncoderBlock(512, 8, 2048)\n",
    "net(torch.randn(8, 30, 512), torch.randn(8, 1, 30)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cd6d14d-ebe2-4bec-89ab-2045d67877ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 30, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.norm_3 = Norm(d_model)\n",
    "        \n",
    "        self.attn_1 = MultiheadAttention(n_heads, d_model, dropout)\n",
    "        self.attn_2 = MultiheadAttention(n_heads, d_model, dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x: tensor input of target batch sentences\n",
    "            shape `(batch_size, seq_len, d_model)`\n",
    "        encoder_output: tensor output (contextual embedding) of encoder block\n",
    "            shape `(batch_size, seq_len, d_model)`\n",
    "        src_mask: tensor mask for encoder output\n",
    "            shape `(batch_size, 1, seq_len)`\n",
    "        tgt_mask: tensor for hide the future represented of predicted token from current step\n",
    "            shape `(batch_size, 1, seq_len)`\n",
    "        Return:\n",
    "        -------\n",
    "        out: tensor, contextual embedding of sentence\n",
    "            shape `(batch_size, seq_len, d_model)`\n",
    "        \"\"\"\n",
    "        x_norm = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn_1(x_norm, x_norm, x_norm, tgt_mask))\n",
    "        \n",
    "        # get corr between current token embedding of decoder with all token embedding from encoder\n",
    "        x_norm = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.attn_2(x_norm, encoder_output, encoder_output, src_mask))\n",
    "        \n",
    "        x_norm = self.norm_3(x)\n",
    "        x = x + self.dropout_3(self.ff(x_norm))\n",
    "        return x\n",
    "net = DecoderBlock(512, 8, 2048)\n",
    "net(torch.randn(8, 30, 512), torch.randn(8, 30, 512), torch.randn(8, 1, 30), torch.randn(8, 1, 30)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425985a5-3f34-4a18-8d34-4f43f3833852",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4831a114-849b-4e8a-a8b0-7a11862b5c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "271d551b-26cc-48b4-8164-63904735132e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 30, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, n_heads, d_ff, num_layer, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.N = num_layer\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PosisionalEncoder(d_model, max_seq_len, dropout)\n",
    "        self.layers = get_clones(EncoderBlock(d_model, n_heads, d_ff, dropout), num_layer)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x: tensor, token idx of input sents\n",
    "            shape `(batch_size, seq_len)`\n",
    "        mask: tensor, shape `(batch_size, 1, seq_len)`\n",
    "        Return:\n",
    "        -------\n",
    "        out: tensor, shape `(batch_size, seq_len, d_model)`\n",
    "        \"\"\"\n",
    "        out = self.embed(x)\n",
    "        out = self.pe(out)\n",
    "        for i in range(self.N):\n",
    "            out = self.layers[i](out, mask)\n",
    "        out = self.norm(out)\n",
    "        return out\n",
    "en_vocab_size, max_seq_len, d_model, n_heads, d_ff, num_layer = 256, 30, 512, 8, 2048, 6\n",
    "net = Encoder(en_vocab_size, max_seq_len, d_model, n_heads, d_ff, num_layer)\n",
    "net(torch.LongTensor(8, max_seq_len).random_(0, en_vocab_size), torch.rand(8, 1, max_seq_len)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bf6d4a7-169c-4d84-a6b6-7cd9d70b4d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 30, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, n_heads, d_ff, num_layer, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.N = num_layer\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PosisionalEncoder(d_model, max_seq_len, dropout)\n",
    "        self.layers = get_clones(DecoderBlock(d_model, n_heads, d_ff, dropout), num_layer)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x: tensor, token idx of input sents\n",
    "            shape `(batch_size, seq_len)`\n",
    "        encoder_output: tensor, contextual embedding for input sents\n",
    "            shape `(batch_size, seq_len, d_model)`\n",
    "        src_mask, tgt_mask: tensor, shape `(batch_size, 1, seq_len)`\n",
    "            2 mask for sentence decoder and encoder respectively\n",
    "        Return:\n",
    "        -------\n",
    "        out: contextual embedding of whole predicted sents\n",
    "        \"\"\"\n",
    "        out = self.embed(x)\n",
    "        out = self.pe(out)\n",
    "        for i in range(self.N):\n",
    "            out = self.layers[i](out, encoder_output, src_mask, tgt_mask)\n",
    "        out = self.norm(out)\n",
    "        return out\n",
    "de_vocab_size, max_seq_len, d_model, n_heads, d_ff, num_layer = 256, 30, 512, 8, 2048, 6\n",
    "net = Decoder(de_vocab_size, max_seq_len, d_model, n_heads, d_ff, num_layer)\n",
    "net(torch.LongTensor(8, max_seq_len).random_(0, de_vocab_size), torch.rand(8, max_seq_len, d_model), torch.randn(8, 1, max_seq_len), torch.randn(8, 1, max_seq_len)).shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "865a00fa-0449-4cb9-8659-6a3e3b27fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, en_config, de_config):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(**en_config)\n",
    "        self.decoder = Decoder(**de_config)\n",
    "        self.fc = nn.Linear(de_config[\"d_model\"], de_config[\"vocab_size\"])\n",
    "    def forward(self, src_sent, tgt_sent, src_mask, tgt_mask):\n",
    "        encoder_output = self.encoder(src_sent, src_mask)\n",
    "        decoder_output = self.decoder(tgt_sent, encoder_output, src_mask, tgt_mask)\n",
    "        out = self.fc(decoder_output)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae05c24d-3f2c-47e7-80b6-f8eec6415f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_config = {\n",
    "    \"vocab_size\": 256,\n",
    "    \"max_seq_len\": 30,\n",
    "    \"d_model\": 512,\n",
    "    \"n_heads\": 8,\n",
    "    \"d_ff\": 2048 ,\n",
    "    \"num_layer\": 6}\n",
    "de_config = {\n",
    "    \"vocab_size\": 128,\n",
    "    \"max_seq_len\": 18,\n",
    "    \"d_model\": 512,\n",
    "    \"n_heads\": 8,\n",
    "    \"d_ff\": 2048 ,\n",
    "    \"num_layer\": 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdb47aab-0db8-43d3-9aa4-8bd9c6c08140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44402816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 18, 128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "en_seq_len = en_config[\"max_seq_len\"]\n",
    "de_seq_len = de_config[\"max_seq_len\"]\n",
    "en_vocab_size = en_config[\"vocab_size\"]\n",
    "de_vocab_size = de_config[\"vocab_size\"]\n",
    "\n",
    "net = Transformer(en_config, de_config)\n",
    "print(sum(p.numel() for p in net.parameters() if p.requires_grad))\n",
    "\n",
    "prob_map = net(src_sent=torch.LongTensor(batch_size, en_seq_len).random_(0, en_vocab_size),\\\n",
    "                tgt_sent=torch.LongTensor(batch_size, de_seq_len).random_(0, de_vocab_size),\\\n",
    "                src_mask=torch.randn(batch_size, 1, en_seq_len),\\\n",
    "                tgt_mask=torch.randn(batch_size, 1, de_seq_len))\n",
    "prob_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1496de33-2c84-44a4-a666-79201de77f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9755,  0.2224,  0.9287,  0.0244,  0.2923, -0.2626,  0.1573,  0.2836,\n",
       "        -0.1178,  0.0238, -0.1071, -0.2879, -0.8419,  0.3327, -1.4278, -0.0689,\n",
       "        -0.3479, -0.0578,  0.8049,  0.0503, -0.2636, -0.1771, -0.2160, -0.1603,\n",
       "        -0.8581,  0.7816, -0.2043, -0.2255,  0.1166,  0.2672, -0.4785,  1.2773,\n",
       "        -0.5677, -0.8595, -0.6009, -0.3740,  0.5364, -1.1635, -1.0674,  0.2244,\n",
       "         0.5846, -1.0198, -0.5836, -0.1003,  0.3574, -0.5686, -0.5007, -0.6308,\n",
       "        -0.5512,  0.1604, -0.8893, -0.8032, -0.3839, -0.1801, -0.9138, -0.5978,\n",
       "        -0.1915, -0.7484, -0.9814,  0.3790,  0.4067, -0.2287, -0.6264, -0.2831,\n",
       "         0.7580,  1.2947, -0.4937, -1.1627, -0.1759, -0.0814,  0.0233, -0.3918,\n",
       "         0.5818, -0.3665,  0.1917,  0.1112, -0.1125,  0.8809, -0.3027,  1.2463,\n",
       "         0.9553,  1.2005, -0.0825, -0.7323, -0.7041,  0.3189, -0.5870,  0.7770,\n",
       "         0.7898, -0.0901,  0.0709,  0.2313, -1.0618,  0.0791, -0.2431, -0.0104,\n",
       "         0.6020,  0.1511, -0.4289,  0.0944,  0.6562,  0.1368, -1.5386, -0.2492,\n",
       "        -1.6648,  0.5203,  0.9154,  1.0152, -0.0169,  0.1024,  0.0547, -0.4830,\n",
       "        -0.2235, -0.5197,  1.6533, -0.0734, -0.0364, -0.3216,  0.5662, -0.2507,\n",
       "         0.5869,  0.3009,  0.5252,  0.4940, -0.3398, -0.5734, -0.5854, -0.6870],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_map[0,0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2cc2d9-b1ea-44cb-887a-624803855d99",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca3ce18-bd51-4d3b-9389-6a961b43721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"nam194/vietnews\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d0f66b6a-aa0d-4930-a0e0-3f0943faa3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator\n",
    "\n",
    "class MyIterator(Iterator):\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool(d, random_shuffler):\n",
    "                for p in data.batch(d, self.batch_size * 100):\n",
    "                    p_batch = data.batch(\n",
    "                        sorted(p, key=self.sort_key),\n",
    "                        self.batch_size, self.batch_size_fn)\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "\n",
    "            self.batches = pool(self.data(), self.random_shuffler)\n",
    "\n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size,\n",
    "                                self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))\n",
    "\n",
    "\n",
    "global max_src_in_batch, max_tgt_in_batch\n",
    "\n",
    "\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch, len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch, len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c6770b9c-6041-43b4-8606-85293f92d6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nopeak_mask(size, device):\n",
    "    \"\"\"Tạo mask được sử dụng trong decoder để lúc dự đoán trong quá trình huấn luyện\n",
    "     mô hình không nhìn thấy được các từ ở tương lai\n",
    "    \"\"\"\n",
    "    np_mask = np.triu(np.ones((1, size, size)),\n",
    "    k=1).astype('uint8')\n",
    "    np_mask =  Variable(torch.from_numpy(np_mask) == 0)\n",
    "    np_mask = np_mask.to(device)\n",
    "    \n",
    "    return np_mask\n",
    "\n",
    "def create_masks(src, trg, src_pad, trg_pad, device):\n",
    "    \"\"\" Tạo mask cho encoder, \n",
    "    để mô hình không bỏ qua thông tin của các kí tự PAD do chúng ta thêm vào \n",
    "    \"\"\"\n",
    "    src_mask = (src != src_pad).unsqueeze(-2)\n",
    "\n",
    "    if trg is not None:\n",
    "        trg_mask = (trg != trg_pad).unsqueeze(-2)\n",
    "        size = trg.size(1) # get seq_len for matrix\n",
    "        np_mask = nopeak_mask(size, device)\n",
    "        if trg.is_cuda:\n",
    "            np_mask.cuda()\n",
    "        trg_mask = trg_mask & np_mask\n",
    "        \n",
    "    else:\n",
    "        trg_mask = None\n",
    "    return src_mask, trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8b3a100c-8ea2-4e1a-81b0-7e513d793e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "\n",
    "def get_synonym(word, SRC):\n",
    "    syns = wordnet.synsets(word)\n",
    "    for s in syns:\n",
    "        for l in s.lemmas():\n",
    "            if SRC.vocab.stoi[l.name()] != 0:\n",
    "                return SRC.vocab.stoi[l.name()]\n",
    "            \n",
    "    return 0\n",
    "\n",
    "def multiple_replace(dict, text):\n",
    "  # Create a regular expression  from the dictionary keys\n",
    "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "\n",
    "  # For each match, look-up corresponding value in dictionary\n",
    "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e725d896-c75d-4755-be91-608d0100e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_vars(src, model, SRC, TRG, device, k, max_len):\n",
    "    \"\"\" Tính toán các ma trận cần thiết trong quá trình translation sau khi mô hình học xong\n",
    "    \"\"\"\n",
    "    init_tok = TRG.vocab.stoi['<sos>']\n",
    "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
    "\n",
    "    # tính sẵn output của encoder \n",
    "    e_output = model.encoder(src, src_mask)\n",
    "    \n",
    "    outputs = torch.LongTensor([[init_tok]])\n",
    "    \n",
    "    outputs = outputs.to(device)\n",
    "    \n",
    "    trg_mask = nopeak_mask(1, device)\n",
    "    # dự đoán kí tự đầu tiên\n",
    "    out = model.out(model.decoder(outputs,\n",
    "    e_output, src_mask, trg_mask))\n",
    "    out = F.softmax(out, dim=-1)\n",
    "    \n",
    "    probs, ix = out[:, -1].data.topk(k)\n",
    "    log_scores = torch.Tensor([math.log(prob) for prob in probs.data[0]]).unsqueeze(0)\n",
    "    \n",
    "    outputs = torch.zeros(k, max_len).long()\n",
    "    outputs = outputs.to(device)\n",
    "    outputs[:, 0] = init_tok\n",
    "    outputs[:, 1] = ix[0]\n",
    "    \n",
    "    e_outputs = torch.zeros(k, e_output.size(-2),e_output.size(-1))\n",
    "   \n",
    "    e_outputs = e_outputs.to(device)\n",
    "    e_outputs[:, :] = e_output[0]\n",
    "    \n",
    "    return outputs, e_outputs, log_scores\n",
    "\n",
    "def k_best_outputs(outputs, out, log_scores, i, k):\n",
    "    \n",
    "    probs, ix = out[:, -1].data.topk(k)\n",
    "    log_probs = torch.Tensor([math.log(p) for p in probs.data.view(-1)]).view(k, -1) + log_scores.transpose(0,1)\n",
    "    k_probs, k_ix = log_probs.view(-1).topk(k)\n",
    "    \n",
    "    row = k_ix // k\n",
    "    col = k_ix % k\n",
    "\n",
    "    outputs[:, :i] = outputs[row, :i]\n",
    "    outputs[:, i] = ix[row, col]\n",
    "\n",
    "    log_scores = k_probs.unsqueeze(0)\n",
    "    \n",
    "    return outputs, log_scores\n",
    "\n",
    "def beam_search(src, model, SRC, TRG, device, k, max_len):    \n",
    "\n",
    "    outputs, e_outputs, log_scores = init_vars(src, model, SRC, TRG, device, k, max_len)\n",
    "    eos_tok = TRG.vocab.stoi['<eos>']\n",
    "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
    "    ind = None\n",
    "    for i in range(2, max_len):\n",
    "    \n",
    "        trg_mask = nopeak_mask(i, device)\n",
    "\n",
    "        out = model.out(model.decoder(outputs[:,:i],\n",
    "        e_outputs, src_mask, trg_mask))\n",
    "\n",
    "        out = F.softmax(out, dim=-1)\n",
    "    \n",
    "        outputs, log_scores = k_best_outputs(outputs, out, log_scores, i, k)\n",
    "        \n",
    "        ones = (outputs==eos_tok).nonzero() # Occurrences of end symbols for all input sentences.\n",
    "        sentence_lengths = torch.zeros(len(outputs), dtype=torch.long).cuda()\n",
    "        for vec in ones:\n",
    "            i = vec[0]\n",
    "            if sentence_lengths[i]==0: # First end symbol has not been found yet\n",
    "                sentence_lengths[i] = vec[1] # Position of first end symbol\n",
    "\n",
    "        num_finished_sentences = len([s for s in sentence_lengths if s > 0])\n",
    "\n",
    "        if num_finished_sentences == k:\n",
    "            alpha = 0.7\n",
    "            div = 1/(sentence_lengths.type_as(log_scores)**alpha)\n",
    "            _, ind = torch.max(log_scores * div, 1)\n",
    "            ind = ind.data[0]\n",
    "            break\n",
    "    \n",
    "    if ind is None:\n",
    "        \n",
    "        length = (outputs[0]==eos_tok).nonzero()[0] if len((outputs[0]==eos_tok).nonzero()) > 0 else -1\n",
    "        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[0][1:length]])\n",
    "    \n",
    "    else:\n",
    "        length = (outputs[ind]==eos_tok).nonzero()[0]\n",
    "        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[ind][1:length]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4c53922a-c1f3-4800-a9b5-16bf45c21407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, SRC, TRG, device, k, max_len):\n",
    "    \"\"\"Dịch một câu sử dụng beamsearch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    indexed = []\n",
    "    sentence = SRC.preprocess(sentence)\n",
    "    \n",
    "    for tok in sentence:\n",
    "        if SRC.vocab.stoi[tok] != SRC.vocab.stoi['<eos>']:\n",
    "            indexed.append(SRC.vocab.stoi[tok])\n",
    "        else:\n",
    "            indexed.append(get_synonym(tok, SRC))\n",
    "    \n",
    "    sentence = Variable(torch.LongTensor([indexed]))\n",
    "    \n",
    "    sentence = sentence.to(device)\n",
    "    \n",
    "    sentence = beam_search(sentence, model, SRC, TRG, device, k, max_len)\n",
    "\n",
    "    return  multiple_replace({' ?' : '?',' !':'!',' .':'.','\\' ':'\\'',' ,':','}, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0fd4e8e1-80c0-4f23-a4f0-41aac6fa03f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'deprecated' from 'typing_extensions' (C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\typing_extensions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\confection\\__init__.py:38\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, Extra, ValidationError, create_model\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfields\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelField\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydantic\\__init__.py:13\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore_schema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     FieldSerializationInfo,\n\u001b[0;32m      6\u001b[0m     FieldValidationInfo,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     ValidatorFunctionWrapHandler,\n\u001b[0;32m     11\u001b[0m )\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclasses\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_annotated_handlers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     15\u001b[0m     GetCoreSchemaHandler \u001b[38;5;28;01mas\u001b[39;00m GetCoreSchemaHandler,\n\u001b[0;32m     16\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydantic\\dataclasses.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal, dataclass_transform\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _config, _decorators, _typing_extra\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _dataclasses \u001b[38;5;28;01mas\u001b[39;00m _pydantic_dataclasses\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydantic\\_internal\\_config.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal, Self\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfigDict, ExtraValues, JsonEncoder, JsonSchemaExtraCallable\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PydanticUserError\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydantic\\config.py:9\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_migration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getattr_migration\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecated\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseConfig\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecated\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Extra \u001b[38;5;28;01mas\u001b[39;00m _Extra\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydantic\\deprecated\\config.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal, deprecated\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _config\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'deprecated' from 'typing_extensions' (C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\typing_extensions.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mtokenize\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\errors.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\compat.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Helpers for Python and platform compatibility.\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m copy_array\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcPickle\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\__init__.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregistry\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\config.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VARIABLE_RE, Config, ConfigValidationError, Promise\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Decorator\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\confection\\__init__.py:42\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelMetaclass\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, create_model, ValidationError, Extra  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelMetaclass  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfields\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelField  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydantic\\__init__.py:13\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydantic_core\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore_schema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     FieldSerializationInfo,\n\u001b[0;32m      6\u001b[0m     FieldValidationInfo,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     ValidatorFunctionWrapHandler,\n\u001b[0;32m     11\u001b[0m )\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclasses\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_annotated_handlers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     15\u001b[0m     GetCoreSchemaHandler \u001b[38;5;28;01mas\u001b[39;00m GetCoreSchemaHandler,\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_annotated_handlers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     GetJsonSchemaHandler \u001b[38;5;28;01mas\u001b[39;00m GetJsonSchemaHandler,\n\u001b[0;32m     19\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydantic\\dataclasses.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any, Callable, Generic, NoReturn, TypeVar, overload\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal, dataclass_transform\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _config, _decorators, _typing_extra\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _dataclasses \u001b[38;5;28;01mas\u001b[39;00m _pydantic_dataclasses\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_migration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getattr_migration\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydantic\\_internal\\_config.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core_schema\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal, Self\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfigDict, ExtraValues, JsonEncoder, JsonSchemaExtraCallable\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PydanticUserError\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PydanticDeprecatedSince20\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydantic\\config.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal, TypeAlias, TypedDict\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_migration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getattr_migration\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecated\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseConfig\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecated\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Extra \u001b[38;5;28;01mas\u001b[39;00m _Extra\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydantic\\deprecated\\config.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal, deprecated\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _config\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PydanticDeprecatedSince20\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'deprecated' from 'typing_extensions' (C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\typing_extensions.py)"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "\n",
    "class tokenize(object):\n",
    "\n",
    "    def __init__(self, lang):\n",
    "        self.nlp = spacy.load(lang)\n",
    "\n",
    "    def tokenizer(self, sentence):\n",
    "        sentence = re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n",
    "        sentence = re.sub(r\"[ ]+\", \" \", sentence)\n",
    "        sentence = re.sub(r\"\\!+\", \"!\", sentence)\n",
    "        sentence = re.sub(r\"\\,+\", \",\", sentence)\n",
    "        sentence = re.sub(r\"\\?+\", \"?\", sentence)\n",
    "        sentence = sentence.lower()\n",
    "        return [tok.text for tok in self.nlp.tokenizer(sentence) if tok.text != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "da730e71-e0b5-4788-aabd-487a5259e5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing_extensions in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.0.1)\n",
      "Collecting typing_extensions\n",
      "  Obtaining dependency information for typing_extensions from https://files.pythonhosted.org/packages/ec/6b/63cc3df74987c36fe26157ee12e09e8f9db4de771e0f3404263117e75b95/typing_extensions-4.7.1-py3-none-any.whl.metadata\n",
      "  Using cached typing_extensions-4.7.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: typing_extensions\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.0.1\n",
      "    Not uninstalling typing-extensions at c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages, outside environment C:\\Users\\Admin\\Desktop\\transformer_pointer_generator_network\\venv\n",
      "    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n",
      "Successfully installed typing_extensions-4.7.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.10.0 requires protobuf<4,>=3.12, but you have protobuf 4.24.0 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.7.1 which is incompatible.\n",
      "torchaudio 0.13.1 requires torch==1.13.1, but you have torch 2.0.1 which is incompatible.\n",
      "torchvision 0.14.1 requires torch==1.13.1, but you have torch 2.0.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deprecated in c:\\users\\admin\\desktop\\transformer_pointer_generator_network\\venv\\lib\\site-packages (1.2.14)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from deprecated) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U typing_extensions\n",
    "!pip install -U deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755e9f17-77cc-4678-8f81-d3ea84328fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
