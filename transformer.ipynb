{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8207083d-bb83-4615-bdf4-0fddc08cd9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refer: github.com/pbcquoc\n",
    "import numpy as np\n",
    "import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch, os, math, copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1313ca74-59a2-4b73-ae5a-42eb39121738",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Positional Encoder\n",
    "(this trick makes transformer and variants awesome but how ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fad86056-6cb8-45ec-babd-3b641de32c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 30, 512])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        return embed\n",
    "\n",
    "class PosisionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model=768, max_seq_len=256, dropout=0.1):\n",
    "        super(PosisionalEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0,d_model,2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** (2*i/d_model)))\n",
    "                pe[pos, i+1] = math.cos(pos / (10000 ** (2*i/d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # this makes pe is not trained/updated by optimizer\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        seq_len = x.size(1)\n",
    "        pe = Variable(self.pe[:, :seq_len], requires_grad=False)\n",
    "        if x.is_cuda:\n",
    "            pe.cuda()\n",
    "        x = self.dropout(x + pe)\n",
    "        return x\n",
    "PosisionalEncoder(512)(torch.rand(5, 30, 512)).shape    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a24b3-686c-45bd-b0c1-aaae84cbb9f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Multihead Attention operator\n",
    "(awesome feature extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a38eb69-a8ff-43c1-93e3-c58e8c55b3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 30, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, dropout=None):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.dropout = nn.Dropout(dropout) if dropout else None\n",
    "        \n",
    "        # init mattrix weights for key, query and value\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        q: tensor shape `(batch_size, seq_len, d_model)`\n",
    "        k: tensor shape `(batch_size, seq_len, d_model)`\n",
    "        v: tensor shape `(batch_size, seq_len, d_model)`\n",
    "        mask: tensor shape `(batch_size, 1, seq_len)`, the mask of self-attn layer at Decoder\n",
    "        Return:\n",
    "        -------\n",
    "        output: tensor shape `(batch_size, seq_len, d_model)`\n",
    "        \"\"\"\n",
    "        # calculate query, key, value vector from weight mattrix\n",
    "        batch_size = q.size(0)\n",
    "        q = self.q_linear(q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.k_linear(k).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.v_linear(v).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # perfrom scale-dot attention op\n",
    "        score = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None: \n",
    "            mask = mask.unsqueeze(1)\n",
    "            score = score.masked_fill(mask==0, -1e9)\n",
    "        score = F.softmax(score, -1)\n",
    "        if self.dropout: \n",
    "            output = self.dropout(score)\n",
    "        output = torch.matmul(score, v)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.out(output)\n",
    "        return output\n",
    "\n",
    "MultiheadAttention(8, 512, 0.1)(torch.rand(8, 30, 512), torch.rand(8, 30, 512), torch.rand(8, 30, 512)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01c8e18-d1a1-47bf-a449-5a8d3ed4cce0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Residual connection and Layer normalization\n",
    "(faster converge and avoid losing information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd988189-8a46-4936-91c4-6cabd715dd24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 128, 512])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(d_model))\n",
    "        self.bias = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm\n",
    "\n",
    "Norm(512)(torch.randn(8, 128, 512)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b937aaa2-4f13-4bf1-8f03-915c824bf435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 128, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model=512, d_ff=2048, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model))\n",
    "    def forward(self, x):\n",
    "        out = self.ff(x)\n",
    "        return out\n",
    "FeedForward()(torch.randn(8,128,512)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe10e05-0d5b-4691-9e27-569f669d34e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Encoder, Decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5500d808-c2bd-4b9b-9a45-f146a3570fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 30, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.attn = MultiheadAttention(n_heads, d_model, dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x: tensor shape `(batch_size, seq_len, model_dim)`\n",
    "        mask: tensor shape `(batch_size, 1, model_dim)` for mask self-attention\n",
    "        Return:\n",
    "        -------\n",
    "        out: tensor shape `(batch_size, seq_len, model_dim)`\n",
    "        \"\"\"\n",
    "        x_norm = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x_norm, x_norm, x_norm, mask))\n",
    "        x_norm = self.norm_2(x)\n",
    "        x = x = self.dropout_2(self.ff(x_norm))\n",
    "        return x\n",
    "net = EncoderBlock(512, 8, 2048)\n",
    "net(torch.randn(8, 30, 512), torch.randn(8, 1, 30)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cd6d14d-ebe2-4bec-89ab-2045d67877ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 30, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.norm_3 = Norm(d_model)\n",
    "        \n",
    "        self.attn_1 = MultiheadAttention(n_heads, d_model, dropout)\n",
    "        self.attn_2 = MultiheadAttention(n_heads, d_model, dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x: tensor input of target batch sentences\n",
    "            shape `(batch_size, seq_len, d_model)`\n",
    "        encoder_output: tensor output (contextual embedding) of encoder block\n",
    "            shape `(batch_size, seq_len, d_model)`\n",
    "        src_mask: tensor mask for encoder output\n",
    "            shape `(batch_size, 1, seq_len)`\n",
    "        tgt_mask: tensor for hide the future represented of predicted token from current step\n",
    "            shape `(batch_size, 1, seq_len)`\n",
    "        Return:\n",
    "        -------\n",
    "        out: tensor, contextual embedding of sentence\n",
    "            shape `(batch_size, seq_len, d_model)`\n",
    "        \"\"\"\n",
    "        x_norm = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn_1(x_norm, x_norm, x_norm, tgt_mask))\n",
    "        \n",
    "        # get corr between current token embedding of decoder with all token embedding from encoder\n",
    "        x_norm = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.attn_2(x_norm, encoder_output, encoder_output, src_mask))\n",
    "        \n",
    "        x_norm = self.norm_3(x)\n",
    "        x = x + self.dropout_3(self.ff(x_norm))\n",
    "        return x\n",
    "net = DecoderBlock(512, 8, 2048)\n",
    "net(torch.randn(8, 30, 512), torch.randn(8, 30, 512), torch.randn(8, 1, 30), torch.randn(8, 1, 30)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425985a5-3f34-4a18-8d34-4f43f3833852",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Build Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4831a114-849b-4e8a-a8b0-7a11862b5c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "271d551b-26cc-48b4-8164-63904735132e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 30, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, n_heads, d_ff, num_layer, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.N = num_layer\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PosisionalEncoder(d_model, max_seq_len, dropout)\n",
    "        self.layers = get_clones(EncoderBlock(d_model, n_heads, d_ff, dropout), num_layer)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x: tensor, token idx of input sents\n",
    "            shape `(batch_size, seq_len)`\n",
    "        mask: tensor, shape `(batch_size, 1, seq_len)`\n",
    "        Return:\n",
    "        -------\n",
    "        out: tensor, shape `(batch_size, seq_len, d_model)`\n",
    "        \"\"\"\n",
    "        out = self.embed(x)\n",
    "        out = self.pe(out)\n",
    "        for i in range(self.N):\n",
    "            out = self.layers[i](out, mask)\n",
    "        out = self.norm(out)\n",
    "        return out\n",
    "en_vocab_size, max_seq_len, d_model, n_heads, d_ff, num_layer = 256, 30, 512, 8, 2048, 6\n",
    "net = Encoder(en_vocab_size, max_seq_len, d_model, n_heads, d_ff, num_layer)\n",
    "net(torch.LongTensor(8, max_seq_len).random_(0, en_vocab_size), torch.rand(8, 1, max_seq_len)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bf6d4a7-169c-4d84-a6b6-7cd9d70b4d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 30, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_len, d_model, n_heads, d_ff, num_layer, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.N = num_layer\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PosisionalEncoder(d_model, max_seq_len, dropout)\n",
    "        self.layers = get_clones(DecoderBlock(d_model, n_heads, d_ff, dropout), num_layer)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x: tensor, token idx of input sents\n",
    "            shape `(batch_size, seq_len)`\n",
    "        encoder_output: tensor, contextual embedding for input sents\n",
    "            shape `(batch_size, seq_len, d_model)`\n",
    "        src_mask: tensor, shape `(batch_size, 1, seq_len)`\n",
    "            mask for sentence embedding of encoder, the \n",
    "            source mask is created by checking where the source sequence \n",
    "            is not equal to a <pad> token. It is 1 where the token is \n",
    "            not a <pad> token and 0 when it is. \n",
    "        tgt_mask: tensor, shape `(batch_size, 1, seq_len)`\n",
    "            mask for token prediction step by step. At step t, the first t\n",
    "            element is the token index, the remain is set to zero.\n",
    "        Return:\n",
    "        -------\n",
    "        out: contextual embedding of whole predicted sents\n",
    "        \"\"\"\n",
    "        out = self.embed(x)\n",
    "        out = self.pe(out)\n",
    "        for i in range(self.N):\n",
    "            out = self.layers[i](out, encoder_output, src_mask, tgt_mask)\n",
    "        out = self.norm(out)\n",
    "        return out\n",
    "de_vocab_size, max_seq_len, d_model, n_heads, d_ff, num_layer = 256, 30, 512, 8, 2048, 6\n",
    "net = Decoder(de_vocab_size, max_seq_len, d_model, n_heads, d_ff, num_layer)\n",
    "net(torch.LongTensor(8, max_seq_len).random_(0, de_vocab_size), torch.rand(8, max_seq_len, d_model), torch.randn(8, 1, max_seq_len), torch.randn(8, 1, max_seq_len)).shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "865a00fa-0449-4cb9-8659-6a3e3b27fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, en_config, de_config):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(**en_config)\n",
    "        self.decoder = Decoder(**de_config)\n",
    "        self.fc = nn.Linear(de_config[\"d_model\"], de_config[\"vocab_size\"])\n",
    "    def forward(self, src_sent, tgt_sent, src_mask, tgt_mask):\n",
    "        encoder_output = self.encoder(src_sent, src_mask)\n",
    "        decoder_output = self.decoder(tgt_sent, encoder_output, src_mask, tgt_mask)\n",
    "        out = self.fc(decoder_output)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae05c24d-3f2c-47e7-80b6-f8eec6415f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_config = {\n",
    "    \"vocab_size\": 256,\n",
    "    \"max_seq_len\": 30,\n",
    "    \"d_model\": 512,\n",
    "    \"n_heads\": 8,\n",
    "    \"d_ff\": 2048 ,\n",
    "    \"num_layer\": 6}\n",
    "de_config = {\n",
    "    \"vocab_size\": 128,\n",
    "    \"max_seq_len\": 18,\n",
    "    \"d_model\": 512,\n",
    "    \"n_heads\": 8,\n",
    "    \"d_ff\": 2048 ,\n",
    "    \"num_layer\": 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdb47aab-0db8-43d3-9aa4-8bd9c6c08140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44402816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 18, 128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "en_seq_len = en_config[\"max_seq_len\"]\n",
    "de_seq_len = de_config[\"max_seq_len\"]\n",
    "en_vocab_size = en_config[\"vocab_size\"]\n",
    "de_vocab_size = de_config[\"vocab_size\"]\n",
    "\n",
    "net = Transformer(en_config, de_config)\n",
    "print(sum(p.numel() for p in net.parameters() if p.requires_grad))\n",
    "\n",
    "prob_map = net(src_sent=torch.LongTensor(batch_size, en_seq_len).random_(0, en_vocab_size),\\\n",
    "                tgt_sent=torch.LongTensor(batch_size, de_seq_len).random_(0, de_vocab_size),\\\n",
    "                src_mask=torch.randn(batch_size, 1, en_seq_len),\\\n",
    "                tgt_mask=torch.randn(batch_size, 1, de_seq_len))\n",
    "prob_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1496de33-2c84-44a4-a666-79201de77f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9755,  0.2224,  0.9287,  0.0244,  0.2923, -0.2626,  0.1573,  0.2836,\n",
       "        -0.1178,  0.0238, -0.1071, -0.2879, -0.8419,  0.3327, -1.4278, -0.0689,\n",
       "        -0.3479, -0.0578,  0.8049,  0.0503, -0.2636, -0.1771, -0.2160, -0.1603,\n",
       "        -0.8581,  0.7816, -0.2043, -0.2255,  0.1166,  0.2672, -0.4785,  1.2773,\n",
       "        -0.5677, -0.8595, -0.6009, -0.3740,  0.5364, -1.1635, -1.0674,  0.2244,\n",
       "         0.5846, -1.0198, -0.5836, -0.1003,  0.3574, -0.5686, -0.5007, -0.6308,\n",
       "        -0.5512,  0.1604, -0.8893, -0.8032, -0.3839, -0.1801, -0.9138, -0.5978,\n",
       "        -0.1915, -0.7484, -0.9814,  0.3790,  0.4067, -0.2287, -0.6264, -0.2831,\n",
       "         0.7580,  1.2947, -0.4937, -1.1627, -0.1759, -0.0814,  0.0233, -0.3918,\n",
       "         0.5818, -0.3665,  0.1917,  0.1112, -0.1125,  0.8809, -0.3027,  1.2463,\n",
       "         0.9553,  1.2005, -0.0825, -0.7323, -0.7041,  0.3189, -0.5870,  0.7770,\n",
       "         0.7898, -0.0901,  0.0709,  0.2313, -1.0618,  0.0791, -0.2431, -0.0104,\n",
       "         0.6020,  0.1511, -0.4289,  0.0944,  0.6562,  0.1368, -1.5386, -0.2492,\n",
       "        -1.6648,  0.5203,  0.9154,  1.0152, -0.0169,  0.1024,  0.0547, -0.4830,\n",
       "        -0.2235, -0.5197,  1.6533, -0.0734, -0.0364, -0.3216,  0.5662, -0.2507,\n",
       "         0.5869,  0.3009,  0.5252,  0.4940, -0.3398, -0.5734, -0.5854, -0.6870],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_map[0,0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2cc2d9-b1ea-44cb-887a-624803855d99",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca3ce18-bd51-4d3b-9389-6a961b43721a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5a49ed74914410a46cf9f9542cce44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/748 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8ad4f898844844b396c2909ef747f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e2bfe90cc540e8bca45bc8af963f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/38.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873ff8379706415c9dfde35e3cc674db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/170M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec5247c43d6453e8251b8a81d810934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/38.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52bb1a21f47b49678e6e862b8a80e3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c6a60c22214ef2b9949c00f854e30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/22498 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f769bdfb09049cc82b43f21f5420adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/99134 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c69a446f1c6f490587dea064f097e42c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/22184 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['guid', 'title', 'abstract', 'article'],\n",
       "        num_rows: 22498\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['guid', 'title', 'abstract', 'article'],\n",
       "        num_rows: 99134\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['guid', 'title', 'abstract', 'article'],\n",
       "        num_rows: 22184\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"nam194/vietnews\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d0f66b6a-aa0d-4930-a0e0-3f0943faa3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator\n",
    "\n",
    "class MyIterator(Iterator):\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool(d, random_shuffler):\n",
    "                for p in data.batch(d, self.batch_size * 100):\n",
    "                    p_batch = data.batch(\n",
    "                        sorted(p, key=self.sort_key),\n",
    "                        self.batch_size, self.batch_size_fn)\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "\n",
    "            self.batches = pool(self.data(), self.random_shuffler)\n",
    "\n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size,\n",
    "                                self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))\n",
    "\n",
    "\n",
    "global max_src_in_batch, max_tgt_in_batch\n",
    "\n",
    "\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch, len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch, len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6770b9c-6041-43b4-8606-85293f92d6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nopeak_mask(size, device):\n",
    "    \"\"\"Tạo mask được sử dụng trong decoder để lúc dự đoán trong quá trình huấn luyện\n",
    "     mô hình không nhìn thấy được các từ ở tương lai\n",
    "    \"\"\"\n",
    "    np_mask = np.triu(np.ones((1, size, size)),\n",
    "    k=1).astype('uint8')\n",
    "    np_mask =  Variable(torch.from_numpy(np_mask) == 0)\n",
    "    np_mask = np_mask.to(device)\n",
    "    \n",
    "    return np_mask\n",
    "\n",
    "def create_masks(src, trg, src_pad, trg_pad, device):\n",
    "    \"\"\" Tạo mask cho encoder, \n",
    "    để mô hình không bỏ qua thông tin của các kí tự PAD do chúng ta thêm vào \n",
    "    \"\"\"\n",
    "    src_mask = (src != src_pad).unsqueeze(-2)\n",
    "\n",
    "    if trg is not None:\n",
    "        trg_mask = (trg != trg_pad).unsqueeze(-2)\n",
    "        size = trg.size(1) # get seq_len for matrix\n",
    "        np_mask = nopeak_mask(size, device)\n",
    "        if trg.is_cuda:\n",
    "            np_mask.cuda()\n",
    "        trg_mask = trg_mask & np_mask\n",
    "        \n",
    "    else:\n",
    "        trg_mask = None\n",
    "    return src_mask, trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b3a100c-8ea2-4e1a-81b0-7e513d793e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "\n",
    "def get_synonym(word, SRC):\n",
    "    syns = wordnet.synsets(word)\n",
    "    for s in syns:\n",
    "        for l in s.lemmas():\n",
    "            if SRC.vocab.stoi[l.name()] != 0:\n",
    "                return SRC.vocab.stoi[l.name()]\n",
    "            \n",
    "    return 0\n",
    "\n",
    "def multiple_replace(dict, text):\n",
    "  # Create a regular expression  from the dictionary keys\n",
    "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "\n",
    "  # For each match, look-up corresponding value in dictionary\n",
    "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e725d896-c75d-4755-be91-608d0100e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_vars(src, model, SRC, TRG, device, k, max_len):\n",
    "    \"\"\" Tính toán các ma trận cần thiết trong quá trình translation sau khi mô hình học xong\n",
    "    \"\"\"\n",
    "    init_tok = TRG.vocab.stoi['<sos>']\n",
    "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
    "\n",
    "    # tính sẵn output của encoder \n",
    "    e_output = model.encoder(src, src_mask)\n",
    "    \n",
    "    outputs = torch.LongTensor([[init_tok]])\n",
    "    \n",
    "    outputs = outputs.to(device)\n",
    "    \n",
    "    trg_mask = nopeak_mask(1, device)\n",
    "    # dự đoán kí tự đầu tiên\n",
    "    out = model.out(model.decoder(outputs,\n",
    "    e_output, src_mask, trg_mask))\n",
    "    out = F.softmax(out, dim=-1)\n",
    "    \n",
    "    probs, ix = out[:, -1].data.topk(k)\n",
    "    log_scores = torch.Tensor([math.log(prob) for prob in probs.data[0]]).unsqueeze(0)\n",
    "    \n",
    "    outputs = torch.zeros(k, max_len).long()\n",
    "    outputs = outputs.to(device)\n",
    "    outputs[:, 0] = init_tok\n",
    "    outputs[:, 1] = ix[0]\n",
    "    \n",
    "    e_outputs = torch.zeros(k, e_output.size(-2),e_output.size(-1))\n",
    "   \n",
    "    e_outputs = e_outputs.to(device)\n",
    "    e_outputs[:, :] = e_output[0]\n",
    "    \n",
    "    return outputs, e_outputs, log_scores\n",
    "\n",
    "def k_best_outputs(outputs, out, log_scores, i, k):\n",
    "    \n",
    "    probs, ix = out[:, -1].data.topk(k)\n",
    "    log_probs = torch.Tensor([math.log(p) for p in probs.data.view(-1)]).view(k, -1) + log_scores.transpose(0,1)\n",
    "    k_probs, k_ix = log_probs.view(-1).topk(k)\n",
    "    \n",
    "    row = k_ix // k\n",
    "    col = k_ix % k\n",
    "\n",
    "    outputs[:, :i] = outputs[row, :i]\n",
    "    outputs[:, i] = ix[row, col]\n",
    "\n",
    "    log_scores = k_probs.unsqueeze(0)\n",
    "    \n",
    "    return outputs, log_scores\n",
    "\n",
    "def beam_search(src, model, SRC, TRG, device, k, max_len):    \n",
    "\n",
    "    outputs, e_outputs, log_scores = init_vars(src, model, SRC, TRG, device, k, max_len)\n",
    "    eos_tok = TRG.vocab.stoi['<eos>']\n",
    "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
    "    ind = None\n",
    "    for i in range(2, max_len):\n",
    "    \n",
    "        trg_mask = nopeak_mask(i, device)\n",
    "\n",
    "        out = model.out(model.decoder(outputs[:,:i],\n",
    "        e_outputs, src_mask, trg_mask))\n",
    "\n",
    "        out = F.softmax(out, dim=-1)\n",
    "    \n",
    "        outputs, log_scores = k_best_outputs(outputs, out, log_scores, i, k)\n",
    "        \n",
    "        ones = (outputs==eos_tok).nonzero() # Occurrences of end symbols for all input sentences.\n",
    "        sentence_lengths = torch.zeros(len(outputs), dtype=torch.long).cuda()\n",
    "        for vec in ones:\n",
    "            i = vec[0]\n",
    "            if sentence_lengths[i]==0: # First end symbol has not been found yet\n",
    "                sentence_lengths[i] = vec[1] # Position of first end symbol\n",
    "\n",
    "        num_finished_sentences = len([s for s in sentence_lengths if s > 0])\n",
    "\n",
    "        if num_finished_sentences == k:\n",
    "            alpha = 0.7\n",
    "            div = 1/(sentence_lengths.type_as(log_scores)**alpha)\n",
    "            _, ind = torch.max(log_scores * div, 1)\n",
    "            ind = ind.data[0]\n",
    "            break\n",
    "    \n",
    "    if ind is None:\n",
    "        \n",
    "        length = (outputs[0]==eos_tok).nonzero()[0] if len((outputs[0]==eos_tok).nonzero()) > 0 else -1\n",
    "        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[0][1:length]])\n",
    "    \n",
    "    else:\n",
    "        length = (outputs[ind]==eos_tok).nonzero()[0]\n",
    "        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[ind][1:length]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c53922a-c1f3-4800-a9b5-16bf45c21407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, SRC, TRG, device, k, max_len):\n",
    "    \"\"\"Dịch một câu sử dụng beamsearch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    indexed = []\n",
    "    sentence = SRC.preprocess(sentence)\n",
    "    \n",
    "    for tok in sentence:\n",
    "        if SRC.vocab.stoi[tok] != SRC.vocab.stoi['<eos>']:\n",
    "            indexed.append(SRC.vocab.stoi[tok])\n",
    "        else:\n",
    "            indexed.append(get_synonym(tok, SRC))\n",
    "    \n",
    "    sentence = Variable(torch.LongTensor([indexed]))\n",
    "    \n",
    "    sentence = sentence.to(device)\n",
    "    \n",
    "    sentence = beam_search(sentence, model, SRC, TRG, device, k, max_len)\n",
    "\n",
    "    return  multiple_replace({' ?' : '?',' !':'!',' .':'.','\\' ':'\\'',' ,':','}, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fd4e8e1-80c0-4f23-a4f0-41aac6fa03f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "\n",
    "class tokenize(object):\n",
    "\n",
    "    def __init__(self, lang):\n",
    "        self.nlp = spacy.load(lang)\n",
    "\n",
    "    def tokenizer(self, sentence):\n",
    "        # sentence = re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n",
    "        # sentence = re.sub(r\"[ ]+\", \" \", sentence)\n",
    "        # sentence = re.sub(r\"\\!+\", \"!\", sentence)\n",
    "        # sentence = re.sub(r\"\\,+\", \",\", sentence)\n",
    "        # sentence = re.sub(r\"\\?+\", \"?\", sentence)\n",
    "        # sentence = sentence.lower()\n",
    "        return [tok.text for tok in self.nlp.tokenizer(sentence) if tok.text != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "755e9f17-77cc-4678-8f81-d3ea84328fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dill as pickle\n",
    "import pandas as pd\n",
    "\n",
    "def read_data(dataset, mode, src_field, trg_field):\n",
    "    src_data = dataset[mode][src_field]\n",
    "    trg_data = dataset[mode][trg_field]\n",
    " \n",
    "    return src_data, trg_data\n",
    "\n",
    "def create_fields(src_lang, trg_lang):\n",
    "    \n",
    "    print(\"loading spacy tokenizers...\")\n",
    "    \n",
    "    t_src = tokenize(src_lang)\n",
    "    t_trg = tokenize(trg_lang)\n",
    "\n",
    "    TRG = data.Field(lower=True, tokenize=t_trg.tokenizer, init_token='<sos>', eos_token='<eos>', fix_length=64)\n",
    "    SRC = data.Field(lower=True, tokenize=t_src.tokenizer, fix_length=256)\n",
    "        \n",
    "    return SRC, TRG\n",
    "\n",
    "def create_dataset(src_data, trg_data, max_strlen, batchsize, device, SRC, TRG, istrain=True):\n",
    "    print(\"creating dataset and iterator... \")\n",
    "\n",
    "    raw_data = {'src' : [line for line in src_data], 'trg': [line for line in trg_data]}\n",
    "    df = pd.DataFrame(raw_data, columns=[\"src\", \"trg\"])\n",
    "    \n",
    "    mask = (df['src'].str.count(' ') < max_strlen) & (df['trg'].str.count(' ') < max_strlen)\n",
    "    df = df.loc[mask]\n",
    "\n",
    "    df.to_csv(\"translate_transformer_temp.csv\", index=False)\n",
    "    \n",
    "    data_fields = [('src', SRC), ('trg', TRG)]\n",
    "    train = data.TabularDataset('./translate_transformer_temp.csv', format='csv', fields=data_fields)\n",
    "\n",
    "    train_iter = MyIterator(train, batch_size=batchsize, device=device,\n",
    "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "                        batch_size_fn=batch_size_fn, train=istrain, shuffle=True)\n",
    "    \n",
    "    os.remove('translate_transformer_temp.csv')\n",
    "    \n",
    "    if istrain:\n",
    "        SRC.build_vocab(train)\n",
    "        TRG.build_vocab(train)\n",
    "\n",
    "    return train_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99dd337f-645d-4729-a31c-e1cdc06bd7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(model, optimizer, batch, criterion):\n",
    "    \"\"\"\n",
    "    Một lần cập nhật mô hình\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    src = batch.src.transpose(0,1).cuda()\n",
    "    trg = batch.trg.transpose(0,1).cuda()\n",
    "    trg_input = trg[:, :-1]\n",
    "    src_mask, trg_mask = create_masks(src, trg_input, src_pad, trg_pad, opt['device'])\n",
    "    preds = model(src, trg_input, src_mask, trg_mask)\n",
    "\n",
    "    ys = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(preds.view(-1, preds.size(-1)), ys)\n",
    "    loss.backward()\n",
    "    optimizer.step_and_update_lr()\n",
    "    \n",
    "    loss = loss.item()\n",
    "    \n",
    "    return loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "603d217a-8273-49fe-b87c-6ed8ed641733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validiate(model, valid_iter, criterion):\n",
    "    \"\"\" Tính loss trên tập validation\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        total_loss = []\n",
    "        for batch in valid_iter:\n",
    "            src = batch.src.transpose(0,1).cuda()\n",
    "            trg = batch.trg.transpose(0,1).cuda()\n",
    "            trg_input = trg[:, :-1]\n",
    "            src_mask, trg_mask = create_masks(src, trg_input, src_pad, trg_pad, opt['device'])\n",
    "            preds = model(src, trg_input, src_mask, trg_mask)\n",
    "\n",
    "            ys = trg[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = criterion(preds.view(-1, preds.size(-1)), ys)\n",
    "            \n",
    "            loss = loss.item()\n",
    "            \n",
    "            total_loss.append(loss)\n",
    "        \n",
    "    avg_loss = np.mean(total_loss)\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0132e9f7-4889-458f-8507-e0046e7f1bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledOptim():\n",
    "    '''A simple wrapper class for learning rate scheduling'''\n",
    "\n",
    "    def __init__(self, optimizer, init_lr, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.init_lr = init_lr\n",
    "        self.d_model = d_model\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_steps = 0\n",
    "\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients with the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        d_model = self.d_model\n",
    "        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps\n",
    "        return (d_model ** -0.5) * min(n_steps ** (-0.5), n_steps * n_warmup_steps ** (-1.5))\n",
    "\n",
    "    def state_dict(self):\n",
    "        optimizer_state_dict = {\n",
    "            'init_lr':self.init_lr,\n",
    "            'd_model':self.d_model,\n",
    "            'n_warmup_steps':self.n_warmup_steps,\n",
    "            'n_steps':self.n_steps,\n",
    "            '_optimizer':self._optimizer.state_dict(),\n",
    "        }\n",
    "        \n",
    "        return optimizer_state_dict\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.init_lr = state_dict['init_lr']\n",
    "        self.d_model = state_dict['d_model']\n",
    "        self.n_warmup_steps = state_dict['n_warmup_steps']\n",
    "        self.n_steps = state_dict['n_steps']\n",
    "        \n",
    "        self._optimizer.load_state_dict(state_dict['_optimizer'])\n",
    "        \n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a2f5271-bac4-4122-ab3e-6223edc9388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, padding_idx, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            # true_dist = pred.data.clone()\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 2))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "            true_dist[:, self.padding_idx] = 0\n",
    "            mask = torch.nonzero(target.data == self.padding_idx, as_tuple=False)\n",
    "            if mask.dim() > 0:\n",
    "                true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "            \n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def bleu(valid_src_data, valid_trg_data, model, SRC, TRG, device, k, max_strlen):\n",
    "    pred_sents = []\n",
    "    for sentence in valid_src_data:\n",
    "        pred_trg = translate_sentence(sentence, model, SRC, TRG, device, k, max_strlen)\n",
    "        pred_sents.append(pred_trg)\n",
    "    \n",
    "    pred_sents = [TRG.preprocess(sent) for sent in pred_sents]\n",
    "    trg_sents = [[sent.split()] for sent in valid_trg_data]\n",
    "    \n",
    "    return bleu_score(pred_sents, trg_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9431e69-2379-4235-a968-b0cf21a1c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src_data, train_trg_data = read_data(dataset, \"train\", \"article\", \"abstract\")\n",
    "valid_src_data, valid_trg_data = read_data(dataset, \"validation\", \"article\", \"abstract\")\n",
    "\n",
    "SRC, TRG = create_fields(\"vi_core_news_lg\", \"vi_core_news_lg\")\n",
    "train_iter = create_dataset(train_src_data, train_trg_data, 256, 8, 'cpu', SRC, TRG, istrain=True)\n",
    "valid_iter = create_dataset(valid_src_data, valid_trg_data, 64, 8, 'cpu', SRC, TRG, istrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4e77751-ba50-4be9-9071-d63e0bab1626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>trg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Viện_kiểm_sát nhân_dân T X.Phú Mỹ , tỉnh Bà_Rị...</td>\n",
       "      <td>Ngày 22/6 , cơ_quan CSĐT Công_an T X.Phú Mỹ đã...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ngày 19/9 , TAND tỉnh Đắk_Nông mở phiên_toà sơ...</td>\n",
       "      <td>Theo cáo_trạng , năm 2013 , Tiến và chị Phạm_T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ngày 26/6 , Công_an huyện Quỳnh_Lưu ( Nghệ_An ...</td>\n",
       "      <td>VOV đưa tin , khoảng 9h sáng ngày 25/6 , em Lê...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lực_lượng công_an ở tỉnh Bạc_Liêu đã khống_chế...</td>\n",
       "      <td>Vào_khoảng 15h ngày 24/7 , Thạch_Sà_Kh . ( SN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nguyễn_Văn_Tiến ( 33 tuổi , quê Ninh_Bình ) nh...</td>\n",
       "      <td>Báo Công_Lý đưa tin , ngày 24/12 , TAND TP. Hà...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 src  \\\n",
       "0  Viện_kiểm_sát nhân_dân T X.Phú Mỹ , tỉnh Bà_Rị...   \n",
       "1  Ngày 19/9 , TAND tỉnh Đắk_Nông mở phiên_toà sơ...   \n",
       "2  Ngày 26/6 , Công_an huyện Quỳnh_Lưu ( Nghệ_An ...   \n",
       "3  Lực_lượng công_an ở tỉnh Bạc_Liêu đã khống_chế...   \n",
       "4  Nguyễn_Văn_Tiến ( 33 tuổi , quê Ninh_Bình ) nh...   \n",
       "\n",
       "                                                 trg  \n",
       "0  Ngày 22/6 , cơ_quan CSĐT Công_an T X.Phú Mỹ đã...  \n",
       "1  Theo cáo_trạng , năm 2013 , Tiến và chị Phạm_T...  \n",
       "2  VOV đưa tin , khoảng 9h sáng ngày 25/6 , em Lê...  \n",
       "3  Vào_khoảng 15h ngày 24/7 , Thạch_Sà_Kh . ( SN ...  \n",
       "4  Báo Công_Lý đưa tin , ngày 24/12 , TAND TP. Hà...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d75ea649-7905-4a48-b76a-436f37ea61d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(682, 45, 425, 39)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(min(valid_trg_data).split()), len(min(valid_src_data).split()), len(min(train_trg_data).split()), len(min(train_src_data).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a3a1a428-b8f0-48d5-a413-7f0d0eb017dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m src_pad \u001b[38;5;241m=\u001b[39m \u001b[43mSRC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[38;5;241m.\u001b[39mstoi[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      2\u001b[0m trg_pad \u001b[38;5;241m=\u001b[39m TRG\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mstoi[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "src_pad = SRC.vocab.stoi['<pad>']\n",
    "trg_pad = TRG.vocab.stoi['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fdd028-159f-488e-9385-665c69df3f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
